
# Neural Network Labs

In neural network we basically train and test different data. In this repository I just have implemented some very basic training methods which we commonly use at the very beginning of learning neural network. Here I implemented

1.  Simulation with Concurrent Inputs in a Static Network

2.  Simulation with Sequential Inputs in a Dynamic Network

3.  Simulation with Concurrent Inputs in a Dynamic Network

4.  Incremental Training of Static Networks

5.  Incremental Training with Dynamic Networks

6.  Batch Training with Static Networks

7.  Batch Training with Dynamic Networks

8.  Single Neuron incremental training 

9.  Single Neuron batch training

10.  XOR with multi-layer perceptrons  

11.  Hebbs Learning Hypothesis

12.  Covariance Hypothesis

## Installation

Use the package manager [pip](https://pip.pypa.io/en/stable/) to install foobar.

```bash
pip install numpy
```

## Usage
All the inputs and outputs are followed by the "Neural Network ToolboxTM
User's Guide"
by
Mark Hudson Beale
Martin T. Hagan
Howard B. Demuth

For the XOR follow the inputs which is given in the code's comment section

## Contributing
Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.

Please make sure to update tests as appropriate.

